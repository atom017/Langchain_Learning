{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892671f6-e1fb-4305-bb7e-14b4376fe233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb==0.5.5 langchain-chroma==0.1.2 langchain==0.2.11 langchain-community==0.2.10 langchain-text-splitters==0.2.2 langchain-groq==0.1.6 transformers==4.43.2 sentence-transformers==3.0.1 unstructured==0.15.0 unstructured[pdf]==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e64d74-9e15-4f9a-8e01-e0ed23e9e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c450678-9b60-4ffb-ab30-9e145b8d1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"your_groq_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dce9e16-8fb0-4502-b99e-1e160068c746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sid/jupyter/myenv/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "# laoding the document\n",
    "loader = UnstructuredFileLoader(\"attention_is_all_you_need.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33865f05-7f3e-465a-9702-e0da708a7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into text chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e4b898-dbcb-4ad2-8e9f-df82d778b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c6f7053-7fab-49ec-a2c2-ea650cb2ec81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2db76b22-5387-4d8a-b9f0-28a83c7b83d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "829f1e6f-fa20-4ee2-9a92-3ed9ca360678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15cdf33e-db7a-4127-ab3c-ceaf08368d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sid/jupyter/myenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "777412da-11eb-4283-ac81-5f048f6b3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"doc_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cd58bc0-630d-4bb9-80a8-ba2ff1238739",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=texts,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80bcc19b-b67c-459c-b21c-eb21e842387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4307b78e-1031-46af-9f2a-725931c2f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm from groq\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d28d355b-2b39-46b9-90ad-d9bb4e39a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a qa chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ffdd6b0-24b5-4411-8340-e20aefd6cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the qa chain and get a response for user query\n",
    "query = \"What is the model architecture discussed in this paper?\"\n",
    "response = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ffcd25-3c42-40b2-a08f-db9a94814cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the model architecture discussed in this paper?', 'result': 'The model architecture discussed in this paper is called the Transformer. It consists of an encoder and a decoder, both of which are composed of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder has an additional sub-layer that performs multi-head attention over the output of the encoder stack. The model uses residual connections and layer normalization.', 'source_documents': [Document(metadata={'source': 'attention_is_all_you_need.pdf'}, page_content='2\\n\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\n3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention'), Document(metadata={'source': 'attention_is_all_you_need.pdf'}, page_content='The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\nReferences\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\n\\narXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\n\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\n\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\n\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n10\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\n\\npreprint arXiv:1610.02357, 2016.\\n\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\n\\nnetwork grammars. In Proc. of NAACL, 2016.\\n\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\n\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n\\n[10] Alex Graves. Generating sequences with recurrent neural networks.\\n\\narXiv preprint\\n\\narXiv:1308.0850, 2013.'), Document(metadata={'source': 'attention_is_all_you_need.pdf'}, page_content='∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n†Work performed while at Google Brain. ‡Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n1\\n\\nIntroduction\\n\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].'), Document(metadata={'source': 'attention_is_all_you_need.pdf'}, page_content='13\\n\\nbe\\n\\napplication\\n\\n just\\n\\nbe\\n\\n,\\n\\n just\\n\\njust\\n\\nLaw\\n\\nits\\n\\nThe\\n\\nshould\\n\\nInput-Input Layer5\\n\\nperfect\\n\\nnever\\n\\nwill\\n\\nbut\\n\\n just\\n\\n <pad>\\n\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14\\n\\nLaw\\n\\nperfect\\n\\nThe\\n\\napplication\\n\\nbe\\n\\nbut\\n\\nshould\\n\\n just\\n\\nInput-Input Layer5\\n\\nbe\\n\\nits\\n\\njust\\n\\n just\\n\\nwill\\n\\n,\\n\\nnever\\n\\n just\\n\\n <pad>\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15')]}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1f38d11-e120-4046-a51f-ad626b45de3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model architecture discussed in this paper is called the Transformer. It consists of an encoder and a decoder, both of which are composed of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder has an additional sub-layer that performs multi-head attention over the output of the encoder stack. The model uses residual connections and layer normalization.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3eec3d40-6922-4e8c-b177-41b588e23d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_is_all_you_need.pdf\n"
     ]
    }
   ],
   "source": [
    "print(response[\"source_documents\"][0].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396808b-3e4c-4891-becd-9a35340b342b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "982fac56-97a5-409a-a8f7-d20ddf004731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model architecture discussed in this paper is called the Transformer. It consists of an encoder and a decoder, both of which are composed of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The decoder has an additional sub-layer that performs multi-head attention over the output of the encoder stack. The model uses residual connections and layer normalization to facilitate training.\n",
      "******************************\n",
      "Source Document: attention_is_all_you_need.pdf\n"
     ]
    }
   ],
   "source": [
    "# invoke the qa chain and get a response for user query\n",
    "query = \"What is the model architecture discussed in this paper?\"\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "print(response[\"result\"])\n",
    "print(\"*\"*30)\n",
    "print(\"Source Document:\", response[\"source_documents\"][0].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76206506-1e15-45c2-92f3-891da7ba091d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
